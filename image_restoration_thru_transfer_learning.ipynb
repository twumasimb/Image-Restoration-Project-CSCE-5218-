{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Wmmmo5wl-fO5K0BYs6lTLqDoiGvaqMfd",
      "authorship_tag": "ABX9TyP6g1Sn7wz+SPVGi1vtPfRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twumasimb/Image-Restoration-Project-CSCE-5218-/blob/main/image_restoration_thru_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HvGP6Di-xVN",
        "outputId": "f376100e-b4cf-4423-d44f-4efda3d18aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bringing-Old-Photos-Back-to-Life'...\n",
            "remote: Enumerating objects: 509, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 509 (delta 3), reused 5 (delta 1), pack-reused 501\u001b[K\n",
            "Receiving objects: 100% (509/509), 40.90 MiB | 29.05 MiB/s, done.\n",
            "Resolving deltas: 100% (227/227), done.\n",
            "Cloning into 'GFPGAN'...\n",
            "remote: Enumerating objects: 523, done.\u001b[K\n",
            "remote: Total 523 (delta 0), reused 0 (delta 0), pack-reused 523\u001b[K\n",
            "Receiving objects: 100% (523/523), 5.39 MiB | 19.94 MiB/s, done.\n",
            "Resolving deltas: 100% (264/264), done.\n",
            "Cloning into 'CycleISP'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 234 (delta 23), reused 36 (delta 21), pack-reused 193\u001b[K\n",
            "Receiving objects: 100% (234/234), 12.38 MiB | 29.42 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "# Getting all the models we will be testing\n",
        "\"\"\"\n",
        "Old-photos to Life\n",
        "\"\"\"\n",
        "!git clone https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life.git\n",
        "\n",
        "\"\"\"\n",
        "GFPGAN\n",
        "\"\"\"\n",
        "!git clone https://github.com/TencentARC/GFPGAN.git\n",
        "\n",
        "\"\"\"\n",
        "CycleISP\n",
        "\"\"\"\n",
        "!git clone https://github.com/swz30/CycleISP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bringing Old Images to Life Using VAEs"
      ],
      "metadata": {
        "id": "B5zW_W8nqHUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Bringing-Old-Photos-Back-to-Life/\n",
        "\n",
        "%cd Face_Enhancement/models/networks/\n",
        "!git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "!cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n",
        "%cd ../../../\n",
        "\n",
        "%cd Global/detection_models\n",
        "!git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "!cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n",
        "%cd ../../\n",
        "\n",
        "%cd Face_Detection/\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "%cd ../\n",
        "\n",
        "%cd Face_Enhancement/\n",
        "!wget https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/face_checkpoints.zip\n",
        "!unzip face_checkpoints.zip\n",
        "%cd ../\n",
        "\n",
        "%cd Global/\n",
        "!wget https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/global_checkpoints.zip\n",
        "!unzip global_checkpoints.zip\n",
        "%cd ../\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-on-DCFMvTf",
        "outputId": "db2348eb-1e11-47eb-f7c9-962d1678f0df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Bringing-Old-Photos-Back-to-Life\n",
            "/content/Bringing-Old-Photos-Back-to-Life/Face_Enhancement/models/networks\n",
            "Cloning into 'Synchronized-BatchNorm-PyTorch'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 188 (delta 10), reused 27 (delta 10), pack-reused 161\u001b[K\n",
            "Receiving objects: 100% (188/188), 47.20 KiB | 1.63 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "/content/Bringing-Old-Photos-Back-to-Life\n",
            "/content/Bringing-Old-Photos-Back-to-Life/Global/detection_models\n",
            "Cloning into 'Synchronized-BatchNorm-PyTorch'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 188 (delta 10), reused 27 (delta 10), pack-reused 161\u001b[K\n",
            "Receiving objects: 100% (188/188), 47.20 KiB | 1.57 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "/content/Bringing-Old-Photos-Back-to-Life\n",
            "/content/Bringing-Old-Photos-Back-to-Life/Face_Detection\n",
            "--2023-04-21 03:34:15--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  24.6MB/s    in 2.5s    \n",
            "\n",
            "2023-04-21 03:34:17 (24.6 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n",
            "/content/Bringing-Old-Photos-Back-to-Life\n",
            "/content/Bringing-Old-Photos-Back-to-Life/Face_Enhancement\n",
            "--2023-04-21 03:34:30--  https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/face_checkpoints.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/49cb1e00-e34c-11eb-82bf-3c592a7d16da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230421%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230421T033430Z&X-Amz-Expires=300&X-Amz-Signature=c0d86368c1bdcd7bfe2b7749d2451b54b8a95a3f9f1866934754e9fe6cc24785&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=274594200&response-content-disposition=attachment%3B%20filename%3Dface_checkpoints.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-04-21 03:34:30--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/49cb1e00-e34c-11eb-82bf-3c592a7d16da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230421%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230421T033430Z&X-Amz-Expires=300&X-Amz-Signature=c0d86368c1bdcd7bfe2b7749d2451b54b8a95a3f9f1866934754e9fe6cc24785&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=274594200&response-content-disposition=attachment%3B%20filename%3Dface_checkpoints.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 684354563 (653M) [application/octet-stream]\n",
            "Saving to: ‘face_checkpoints.zip’\n",
            "\n",
            "face_checkpoints.zi 100%[===================>] 652.65M   101MB/s    in 5.8s    \n",
            "\n",
            "2023-04-21 03:34:36 (113 MB/s) - ‘face_checkpoints.zip’ saved [684354563/684354563]\n",
            "\n",
            "Archive:  face_checkpoints.zip\n",
            "   creating: checkpoints/\n",
            "   creating: checkpoints/Setting_9_epoch_100/\n",
            "  inflating: checkpoints/Setting_9_epoch_100/latest_net_G.pth  \n",
            "   creating: checkpoints/FaceSR_512/\n",
            "  inflating: checkpoints/FaceSR_512/latest_net_G.pth  \n",
            "/content/Bringing-Old-Photos-Back-to-Life\n",
            "/content/Bringing-Old-Photos-Back-to-Life/Global\n",
            "--2023-04-21 03:34:46--  https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/global_checkpoints.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/75e69f00-e34c-11eb-9435-335b7429f0a1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230421%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230421T033446Z&X-Amz-Expires=300&X-Amz-Signature=b482884396ce3e05ca0b488794c8386e76a8ba8d51bc8bc0f6e4dd6f0c9bf98e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=274594200&response-content-disposition=attachment%3B%20filename%3Dglobal_checkpoints.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-04-21 03:34:46--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/75e69f00-e34c-11eb-9435-335b7429f0a1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230421%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230421T033446Z&X-Amz-Expires=300&X-Amz-Signature=b482884396ce3e05ca0b488794c8386e76a8ba8d51bc8bc0f6e4dd6f0c9bf98e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=274594200&response-content-disposition=attachment%3B%20filename%3Dglobal_checkpoints.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2036400762 (1.9G) [application/octet-stream]\n",
            "Saving to: ‘global_checkpoints.zip’\n",
            "\n",
            "global_checkpoints. 100%[===================>]   1.90G   117MB/s    in 15s     \n",
            "\n",
            "2023-04-21 03:35:01 (130 MB/s) - ‘global_checkpoints.zip’ saved [2036400762/2036400762]\n",
            "\n",
            "Archive:  global_checkpoints.zip\n",
            "   creating: checkpoints/\n",
            "   creating: checkpoints/restoration/\n",
            "   creating: checkpoints/restoration/VAE_B_scratch/\n",
            "  inflating: checkpoints/restoration/VAE_B_scratch/latest_net_G.pth  \n",
            "  inflating: checkpoints/restoration/VAE_B_scratch/latest_optimizer_G.pth  \n",
            "  inflating: checkpoints/restoration/VAE_B_scratch/latest_optimizer_D.pth  \n",
            "  inflating: checkpoints/restoration/VAE_B_scratch/latest_net_D.pth  \n",
            "   creating: checkpoints/restoration/VAE_A_quality/\n",
            "  inflating: checkpoints/restoration/VAE_A_quality/latest_net_G.pth  \n",
            "  inflating: checkpoints/restoration/VAE_A_quality/latest_net_featD.pth  \n",
            "  inflating: checkpoints/restoration/VAE_A_quality/latest_optimizer_G.pth  \n",
            "  inflating: checkpoints/restoration/VAE_A_quality/latest_optimizer_D.pth  \n",
            "  inflating: checkpoints/restoration/VAE_A_quality/latest_optimizer_featD.pth  \n",
            "  inflating: checkpoints/restoration/VAE_A_quality/latest_net_D.pth  \n",
            "   creating: checkpoints/restoration/mapping_Patch_Attention/\n",
            "  inflating: checkpoints/restoration/mapping_Patch_Attention/latest_net_mapping_net.pth  \n",
            "  inflating: checkpoints/restoration/mapping_Patch_Attention/latest_net_D.pth  \n",
            "   creating: checkpoints/restoration/mapping_quality/\n",
            "  inflating: checkpoints/restoration/mapping_quality/latest_net_mapping_net.pth  \n",
            "  inflating: checkpoints/restoration/mapping_quality/latest_optimizer_mapping_net.pth  \n",
            "  inflating: checkpoints/restoration/mapping_quality/latest_optimizer_D.pth  \n",
            "  inflating: checkpoints/restoration/mapping_quality/latest_net_D.pth  \n",
            "   creating: checkpoints/restoration/mapping_scratch/\n",
            "  inflating: checkpoints/restoration/mapping_scratch/latest_net_mapping_net.pth  \n",
            "  inflating: checkpoints/restoration/mapping_scratch/latest_optimizer_mapping_net.pth  \n",
            "  inflating: checkpoints/restoration/mapping_scratch/latest_optimizer_D.pth  \n",
            "  inflating: checkpoints/restoration/mapping_scratch/latest_net_D.pth  \n",
            "   creating: checkpoints/restoration/VAE_B_quality/\n",
            "  inflating: checkpoints/restoration/VAE_B_quality/latest_net_G.pth  \n",
            "  inflating: checkpoints/restoration/VAE_B_quality/latest_optimizer_G.pth  \n",
            "  inflating: checkpoints/restoration/VAE_B_quality/latest_optimizer_D.pth  \n",
            "  inflating: checkpoints/restoration/VAE_B_quality/latest_net_D.pth  \n",
            "   creating: checkpoints/detection/\n",
            "  inflating: checkpoints/detection/FT_Epoch_latest.pt  \n",
            "/content/Bringing-Old-Photos-Back-to-Life\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (0.15.1+cu118)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (19.24.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (0.19.3)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (1.10)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (6.0)\n",
            "Collecting dominate>=2.3.1\n",
            "  Downloading dominate-2.7.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (4.7.0.72)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PySimpleGUI\n",
            "  Downloading PySimpleGUI-4.60.4-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 14)) (3.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->-r requirements.txt (line 2)) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 4)) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 4)) (2023.3.21)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 4)) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX->-r requirements.txt (line 9)) (3.20.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 14)) (2.8.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 14)) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: PySimpleGUI, tensorboardX, einops, dominate, dill\n",
            "Successfully installed PySimpleGUI-4.60.4 dill-0.3.6 dominate-2.7.0 einops-0.6.1 tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/datasets/\n",
        "input_folder = \"noisy/celeba-100\"\n",
        "%cd /content/datasets/clean/\n",
        "output_folder = \"vae\"\n",
        "\n",
        "import os\n",
        "basepath = os.getcwd()\n",
        "input_path = os.path.join(basepath, input_folder)\n",
        "output_path = os.path.join(basepath, output_folder)\n",
        "os.mkdir(output_path)\n",
        "\n",
        "%cd /content/Bringing-Old-Photos-Back-to-Life\n",
        "!python run.py --input_folder /content/datasets/noisy/celeba-100/ --output_folder /content/datasets/clean/vae/ --GPU -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adwreGIvUIlU",
        "outputId": "1e4022d8-da27-4a52-ac36-f3fdf9987c6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/datasets/'\n",
            "/content\n",
            "[Errno 2] No such file or directory: '/content/datasets/clean/'\n",
            "/content\n",
            "[Errno 2] No such file or directory: '/content/Bringing-Old-Photos-Back-to-Life'\n",
            "/content\n",
            "python3: can't open file '/content/run.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3GH6k1MCZlMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# %cd /content/datasets/\n",
        "# input_folder = \"noisy/celeba-no-noise\"\n",
        "# basepath = os.getcwd()\n",
        "# input_path = os.path.join(basepath, input_folder)\n",
        "\n",
        "# %cd /content/datasets/clean/\n",
        "# output_folder = \"vae-no-noise\"\n",
        "# basepath = os.getcwd()\n",
        "# output_path = os.path.join(basepath, output_folder)\n",
        "# os.mkdir(output_path)\n",
        "\n",
        "# %cd /content/Bringing-Old-Photos-Back-to-Life\n",
        "# !python run.py --input_folder /content/datasets/noisy/celeba-no-noise/ --output_folder /content/datasets/clean/vae-no-noise/ --GPU -1"
      ],
      "metadata": {
        "id": "icj7dQrZomH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/isp /content/CycleISP/pretrained_models"
      ],
      "metadata": {
        "id": "3tOrcmFspS27"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GFPGANs"
      ],
      "metadata": {
        "id": "9ErqWVsMqRaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd GFPGAN/\n",
        "# # We use BasicSR for both training and inference\n",
        "# !pip install basicsr\n",
        "# # We use face detection and face restoration helper in the facexlib package\n",
        "# !pip install facexlib\n",
        "# # Install other depencencies\n",
        "# !pip install -r requirements.txt\n",
        "# !python setup.py develop\n",
        "# !pip install realesrgan  # used for enhancing the background (non-face) regions\n",
        "# # Now we use the V1.3 model for the demo\n",
        "# !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models"
      ],
      "metadata": {
        "id": "cKHWT4sdqgmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# import os\n",
        "# upload_folder = 'inputs/upload'\n",
        "\n",
        "# if os.path.isdir(upload_folder):\n",
        "#     shutil.rmtree(upload_folder)\n",
        "# os.makedirs(upload_folder, exist_ok=True)\n",
        "# shutil.move('inputs/whole_imgs/Blake_Lively.jpg', 'inputs/upload/Blake_Lively.jpg')"
      ],
      "metadata": {
        "id": "FST4xR1ys0Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# %cd /content/datasets/\n",
        "# input_folder = \"noisy/celeba-100\"\n",
        "# basepath = os.getcwd()\n",
        "# input_path = os.path.join(basepath, input_folder)\n",
        "\n",
        "# %cd /content/datasets/clean/\n",
        "# output_folder = \"gfgan\"\n",
        "# basepath = os.getcwd()\n",
        "# output_path = os.path.join(basepath, output_folder)\n",
        "# os.mkdir(output_path)\n",
        "\n",
        "# %cd /content/GFPGAN/\n",
        "# !python inference_gfpgan.py -i  /content/datasets/noisy/celeba-100/ -o /content/datasets/clean/gfgan/ -v 1.3 -s 2 --bg_upsampler realesrgan"
      ],
      "metadata": {
        "id": "-nvSjtrexrfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CycleISP"
      ],
      "metadata": {
        "id": "MUEPkhagZH8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Raw and Clean Images from the dataset\n",
        "%cd /content/CycleISP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SppuhwfsZ2cI",
        "outputId": "224a5486-396b-41a7-e989-30f45f4669ed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CycleISP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am running CycleISP locally"
      ],
      "metadata": {
        "id": "l_MAHIVlrr7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "# import eval_pred\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchmetrics import PeakSignalNoiseRatio\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def image2tensor(image):\n",
        "  \"\"\"\n",
        "  This function takes the path to an image and convert it into a tensor\n",
        "  \"\"\"\n",
        "  image = Image.open(image)\n",
        "  transform = transforms.Compose([\n",
        "      transforms.PILToTensor()\n",
        "  ])\n",
        "  img_tensor = transform(image)\n",
        "  return img_tensor\n",
        "\n",
        "def psnr(generated, input_img):\n",
        "  psnr = PeakSignalNoiseRatio()\n",
        "  preds = torch.tensor(image2tensor(generated))\n",
        "  target = torch.tensor(image2tensor(input_img))\n",
        "  if(target.size() != preds.size()):\n",
        "    preds = F.interpolate(preds, size=target.size())\n",
        "  psnr_val = psnr(preds, target)\n",
        "  return psnr_val.to_numpy()\n",
        "\n",
        "#Evaluate the PSNR of generated image\n",
        "raw_img = '/content/datasets/noisy/celeba-no-noise/Heng_Fan.png'\n",
        "gen_img = '/content/datasets/clean/vae/final_output/Heng_Fan.png'\n",
        "gfpgan_eval = psnr(raw_img, gen_img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "-7lq8G6OxCi9",
        "outputId": "82a727e6-396f-4af3-d971-579537c3a247"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-d7121d9bed31>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  preds = torch.tensor(image2tensor(generated))\n",
            "<ipython-input-59-d7121d9bed31>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(image2tensor(input_img))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-d7121d9bed31>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mraw_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/datasets/noisy/celeba-no-noise/Heng_Fan.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mgen_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/datasets/clean/vae/final_output/Heng_Fan.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgfpgan_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-d7121d9bed31>\u001b[0m in \u001b[0;36mpsnr\u001b[0;34m(generated, input_img)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mpsnr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpsnr_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3867\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3868\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3869\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   3870\u001b[0m                     \u001b[0;34m\"Input and output must have the same number of spatial dimensions, but got \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3871\u001b[0m                     \u001b[0;34mf\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [150] and output size of torch.Size([3, 192, 144]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ddEMPZ6iu0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hdtdh6jlyeC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}